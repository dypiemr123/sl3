{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2261f234-6c24-401d-b4c2-6ea673cbe7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf3a3a9-63b0-46ef-8acc-3c0944868123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\aarya\n",
      "[nltk_data]     admane/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\aarya\n",
      "[nltk_data]     admane/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aarya admane/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\aarya\n",
      "[nltk_data]     admane/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK dependencies\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a45c817-f7a5-4387-b0f3-2d46987172e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "data = {\n",
    "    'Text': [\n",
    "        \"Natural Language Processing is a growing field of AI.\",\n",
    "        \"Preprocessing includes tokenization, stopwords removal and stemming.\",\n",
    "        \"TF-IDF gives weight to important words in text.\",\n",
    "        np.nan,  # Introduce a null value intentionally\n",
    "        \"     \"  # Outlier - whitespace only\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68f8589-6fe5-440a-a497-c0f0a2bb2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove nulls or empty strings\n",
    "df['Text'] = df['Text'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7cd307f-951b-4a52-9ca4-fccacbc229f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 2. Basic Cleaning - Remove whitespace entries ###\n",
    "df['Text'] = df['Text'].str.strip()\n",
    "df = df[df['Text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a646fce0-9e74-49f2-b2f1-b3680f7d5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3. Tokenization ###\n",
    "df['Tokens'] = df['Text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f6aa2c-ca66-488d-8d2c-9e1a244f0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 4. Stop Word Removal ###\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Tokens_NoStop'] = df['Tokens'].apply(lambda x: [word.lower() for word in x if word.lower() not in stop_words and word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba091da4-4817-4b13-8e05-9ad2353a035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 5. POS Tagging ###\n",
    "df['POS'] = df['Tokens_NoStop'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5423ca4b-1d91-413b-9afd-3778f47df7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 6. Stemming and Lemmatization ###\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ece51f-8632-41c5-9c32-c0a7deb9934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Stemmed'] = df['Tokens_NoStop'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "df['Lemmatized'] = df['Tokens_NoStop'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1464f3a0-4666-468b-ba5e-3e5f4603289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. TF-IDF + Normalization ###\n",
    "# Rejoin lemmatized tokens for vectorizer\n",
    "df['Clean_Text'] = df['Lemmatized'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bf292b0-2626-4f48-8d43-7c5aca0b07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(df['Clean_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ed1850-01b5-4e0f-a618-e2fb625ddb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the TF-IDF matrix\n",
    "normalized_tfidf = normalize(tfidf_matrix, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3bec7ce-c78f-4f92-aee8-24ae5bc851b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(normalized_tfidf.toarray(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4571ee2a-2433-4364-859e-747a4a967312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preprocessed Text DataFrame:\n",
      "                                                Text  \\\n",
      "0  Natural Language Processing is a growing field...   \n",
      "1  Preprocessing includes tokenization, stopwords...   \n",
      "2    TF-IDF gives weight to important words in text.   \n",
      "\n",
      "                                       Tokens_NoStop  \\\n",
      "0  [natural, language, processing, growing, field...   \n",
      "1  [preprocessing, includes, tokenization, stopwo...   \n",
      "2            [gives, weight, important, words, text]   \n",
      "\n",
      "                                          Lemmatized  \n",
      "0  [natural, language, processing, growing, field...  \n",
      "1  [preprocessing, includes, tokenization, stopwo...  \n",
      "2              [give, weight, important, word, text]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "### ðŸ§¾ Final Output ###\n",
    "print(\"\\n Preprocessed Text DataFrame:\")\n",
    "print(df[['Text', 'Tokens_NoStop', 'Lemmatized']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71406ecc-953c-4004-a26c-10075a201c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Normalized TF-IDF Matrix:\n",
      "     ai  field  give  growing  important  includes  language  natural  \\\n",
      "0  0.41   0.41  0.00     0.41       0.00      0.00      0.41     0.41   \n",
      "1  0.00   0.00  0.00     0.00       0.00      0.41      0.00     0.00   \n",
      "2  0.00   0.00  0.45     0.00       0.45      0.00      0.00     0.00   \n",
      "\n",
      "   preprocessing  processing  removal  stemming  stopwords  text  \\\n",
      "0           0.00        0.41     0.00      0.00       0.00  0.00   \n",
      "1           0.41        0.00     0.41      0.41       0.41  0.00   \n",
      "2           0.00        0.00     0.00      0.00       0.00  0.45   \n",
      "\n",
      "   tokenization  weight  word  \n",
      "0          0.00    0.00  0.00  \n",
      "1          0.41    0.00  0.00  \n",
      "2          0.00    0.45  0.45  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n Normalized TF-IDF Matrix:\")\n",
    "print(tfidf_df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea9ffe6c-d23e-4ce8-bf6f-ff7692a5c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preprocessed Text DataFrame:\n",
      "                                                Text  \\\n",
      "0  Natural Language Processing is a growing field...   \n",
      "1  Preprocessing includes tokenization, stopwords...   \n",
      "2    TF-IDF gives weight to important words in text.   \n",
      "\n",
      "                                       Tokens_NoStop  \\\n",
      "0  [natural, language, processing, growing, field...   \n",
      "1  [preprocessing, includes, tokenization, stopwo...   \n",
      "2            [gives, weight, important, words, text]   \n",
      "\n",
      "                                          Lemmatized  \\\n",
      "0  [natural, language, processing, growing, field...   \n",
      "1  [preprocessing, includes, tokenization, stopwo...   \n",
      "2              [give, weight, important, word, text]   \n",
      "\n",
      "                                                 POS  \n",
      "0  [(natural, JJ), (language, NN), (processing, V...  \n",
      "1  [(preprocessing, VBG), (includes, VBZ), (token...  \n",
      "2  [(gives, VBZ), (weight, RBS), (important, JJ),...  \n",
      "\n",
      " Normalized TF-IDF Matrix:\n",
      "     ai  field  give  growing  important  includes  language  natural  \\\n",
      "0  0.41   0.41  0.00     0.41       0.00      0.00      0.41     0.41   \n",
      "1  0.00   0.00  0.00     0.00       0.00      0.41      0.00     0.00   \n",
      "2  0.00   0.00  0.45     0.00       0.45      0.00      0.00     0.00   \n",
      "\n",
      "   preprocessing  processing  removal  stemming  stopwords  text  \\\n",
      "0           0.00        0.41     0.00      0.00       0.00  0.00   \n",
      "1           0.41        0.00     0.41      0.41       0.41  0.00   \n",
      "2           0.00        0.00     0.00      0.00       0.00  0.45   \n",
      "\n",
      "   tokenization  weight  word  \n",
      "0          0.00    0.00  0.00  \n",
      "1          0.41    0.00  0.00  \n",
      "2          0.00    0.45  0.45  \n"
     ]
    }
   ],
   "source": [
    "### ðŸ§¾ Final Output ###\n",
    "print(\"\\n Preprocessed Text DataFrame:\")\n",
    "print(df[['Text', 'Tokens_NoStop', 'Lemmatized', 'POS']])\n",
    "\n",
    "print(\"\\n Normalized TF-IDF Matrix:\")\n",
    "print(tfidf_df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811f11c-b3f0-43b7-9297-dc743a98c083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
